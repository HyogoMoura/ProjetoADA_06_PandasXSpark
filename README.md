# ProjetoADA_06_PandasXSpark
Projeto do modulo bigdata do curso de engenharia de dados Santander Codes 2024
> *Turma 11080 - Santander Coders 2024 - Engenharia de Dados*

Desenvolvimento do projeto "CoparaÃ§Ã£o tempo execuÃ§Ã£o ETL PandasXSpark" com o intuito de extrair dados da [Financial Transactions Dataset](https://www.kaggle.com/datasets/computingvictor/transactions-fraud-datasets), transforma e armazenar no formato parquet.

**Todo projeto foi desenvolvido com a linguagem de programaÃ§Ã£o Python no Databricks Community.**

## âœ’ï¸Autores 
- [Alessandra Cruz](https://github.com/alessandracruz)
- [Ãlex Buracosky](https://github.com/aburacosk)
- [Diana Osorio](https://github.com/diana468)
- [Diogo Moura](https://github.com/HyogoMoura)
- [Felipe Zanardo](https://github.com/FelipeBZanardo)
- [Thiago Silva](https://github.com/thiagodemedeiros)

## ğŸ““ Acesso direto aos notebooks no Databricks:

- [Projeto_Bigdata_Spark.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3117382334967596/1095848774740136/1894754445070318/latest.html)

## ğŸ“‹ Enunciado do Projeto

### **Estudo comprativo de tempo de execuÃ§Ã£o do processo de ETL PandsXSpark**

#### Contexto:
>A PyCoders Ltda., cada vez mais especializada no mundo da Engenharia de Dados, foi procurada por uma fintech para desenvolver um projeto de anÃ¡lise de dados.

>A fintech percebeu que muito dos seus processos estÃ£o se tornando lentos pelo uso incorreto de ferramentas! Desde que estÃ¡ se trabalhando com Big Data, uso de bibliotecas como pandas e sklearn tornam a ExtracÃ§Ã£o, Tratamento e Carregamento dos dados (ETL) processos muito lentos, inclusive o treinamento de modelos de machine learning (ML) tem se tornado um processo muito demorado.

>Para lidar com esse problema, foi sugerido fazer uso da biblioteca pyspark, para implementar todo o fluxo de ETL.

#### Objetivo de projeto:
>Como queremos demostrar que de fato a soluÃ§Ã£o proposta traz uma melhora, foi solicitado implementar uma anÃ¡lise comparativa de resultados usando a antiga abordagem (usando pandas e sklearn) e usando a nova proposta de soluÃ§Ã£o (pyspark). Para isso, tome em consideraÃ§Ã£o o seguinte:

>1. Escolha dois conjuntos de dados interessantes, sendo que um deles Ã© pequeno (menos de 10.000 linhas) e o outro bem maior (acima de 1.000.000 linhas). Uma possivel sugestÃ£o seria usar um unico dataset (com muitos dados), e extrair uma pequena proporÃ§Ã£o dos dados desde dataset e considerar essa parte como o dataset menor.

>2. Aplique todas as etapas de ETL nos dois conjuntos de dados usando pandas y pyspark. As etapas incluem: (1) ExtraÃ§Ã£o dos dados, por exemplo de um csv, (2) Tratamento dos dados (limpeza, alteraÃ§Ã£o de nomes de colunas, criaÃ§Ã£o de mais tabelas, transformaÃ§Ã£o nas colunas, etc.), e, (3) Carregamento dos dados (salvar a transformaÃ§Ã£o feita sobre os dados). 

>3. Lembre que cada etapa tem que ser feita usando unicamente pandas/sklearn ou pyspark.

>4. Como o objetivo Ã© fazer uma anÃ¡lise comparativa, tome em consideraÃ§Ã£o o tempo que demora cada etapa, para depois facilitar as comparaÃ§Ãµes. 

## ğŸ“ DescriÃ§Ã£o do Projeto

#### 1. DefiniÃ§Ã£o da estutura da ExtraÃ§Ã£o:
ExtraÃ§Ã£o foi executada utilziando arquivo formato csv presente no kaggle

#### 2. TrasformaÃ§Ã£o executadas:
Tabelas foram saneadas com plipeza de valores nulos ajustado tipo e verificaÃ§o de valores duplicados ou/ e invalidos

#### 3. PersistÃªncia dos dados:
A persistÃªncia dos dados, apos o processo, foi feita atravÃ©s de arquivos parquet.

## ğŸ“º DemonstraÃ§Ã£o

## â˜‘ï¸  PrÃ©-requisitos
- Cadastro no **[Databricks Community](https://www.databricks.com/try-databricks#account)**;

## âš™ï¸ Passo a passo para executar o projeto:
1. Cadastro e login no **[Databricks Community](https://community.cloud.databricks.com/login.html)**;
2. Import de todos os notebooks presentes nesse projeto para o workspace do Databricks:
    - Pode baixar todos os notebooks e importar via browse;
    - Ou pode importar atravÃ©s da URL dos notebooks presentes no item [Acesso direto aos notebooks no Databricks](#-acesso-direto-aos-notebooks-no-databricks);
3. Criar e iniciar o cluster no Databricks;
4. Inicializar os notebooks 'Projeto_Bigdata_Spark' com 'Run All';

## ğŸ› ï¸ Tecnologias Utilizas

* [Databricks Community](https://www.databricks.com/try-databricks#account)
* [Python](https://www.python.org/) - Linguagem de ProgramaÃ§Ã£o
* [Spark](https://spark.apache.org/) - Processamento em cluster
* [Pandas](https://pandas.pydata.org/) - Biblioteca de processamento de dados


## ğŸš¨ Dificuldades


## ğŸ“ˆ Melhorias futuras:
- Estrutura carregamento de dados utilizando formato tabela floco de neve
- Realizar junÃ§Ã£o de tabelas utilziando formato suportado sql



