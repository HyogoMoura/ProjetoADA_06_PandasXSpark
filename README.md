# ProjetoADA_06_PandasXSpark
Projeto do modulo bigdata do curso de engenharia de dados Santander Codes 2024
> *Turma 11080 - Santander Coders 2024 - Engenharia de Dados*

Desenvolvimento do projeto "Copara√ß√£o tempo execu√ß√£o ETL PandasXSpark" com o intuito de extrair dados da [Financial Transactions Dataset](https://www.kaggle.com/datasets/computingvictor/transactions-fraud-datasets), transforma e armazenar no formato parquet.

**Todo projeto foi desenvolvido com a linguagem de programa√ß√£o Python no Databricks Community.**

## ‚úíÔ∏èAutores 
- [Alessandra Cruz](https://github.com/alessandracruz)
- [√Ålex Buracosky](https://github.com/aburacosk)
- [Diana Osorio](https://github.com/diana468)
- [Diogo Moura](https://github.com/HyogoMoura)
- [Felipe Zanardo](https://github.com/FelipeBZanardo)
- [Thiago Silva](https://github.com/thiagodemedeiros)

## üìì Acesso direto aos notebooks no Databricks:

- [Projeto_Bigdata_Spark.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2861679401553498/2986892077130663/7777518843978957/latest.html)

- [Projeto_Bigdata_Pandas.ipynb](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2861679401553498/2986892077130625/7777518843978957/latest.html)

## üìã Enunciado do Projeto

### **Estudo comparativo de tempo de execu√ß√£o do processo de ETL PandsXSpark**

#### Contexto:
>A PyCoders Ltda., cada vez mais especializada no mundo da Engenharia de Dados, foi procurada por uma fintech para desenvolver um projeto de an√°lise de dados.

>A fintech percebeu que muito dos seus processos est√£o se tornando lentos pelo uso incorreto de ferramentas! Desde que est√° se trabalhando com Big Data, uso de bibliotecas como pandas e sklearn tornam a Extrac√ß√£o, Tratamento e Carregamento dos dados (ETL) processos muito lentos, inclusive o treinamento de modelos de machine learning (ML) tem se tornado um processo muito demorado.

>Para lidar com esse problema, foi sugerido fazer uso da biblioteca pyspark, para implementar todo o fluxo de ETL.

#### Objetivo de projeto:
>Como queremos demostrar que de fato a solu√ß√£o proposta traz uma melhora, foi solicitado implementar uma an√°lise comparativa de resultados usando a antiga abordagem (usando pandas e sklearn) e usando a nova proposta de solu√ß√£o (pyspark). Para isso, tome em considera√ß√£o o seguinte:

>1. Escolha dois conjuntos de dados interessantes, sendo que um deles √© pequeno (menos de 10.000 linhas) e o outro bem maior (acima de 1.000.000 linhas). Uma possivel sugest√£o seria usar um unico dataset (com muitos dados), e extrair uma pequena propor√ß√£o dos dados desde dataset e considerar essa parte como o dataset menor.

>2. Aplique todas as etapas de ETL nos dois conjuntos de dados usando pandas y pyspark. As etapas incluem: (1) Extra√ß√£o dos dados, por exemplo de um csv, (2) Tratamento dos dados (limpeza, altera√ß√£o de nomes de colunas, cria√ß√£o de mais tabelas, transforma√ß√£o nas colunas, etc.), e, (3) Carregamento dos dados (salvar a transforma√ß√£o feita sobre os dados). 

>3. Lembre que cada etapa tem que ser feita usando unicamente pandas/sklearn ou pyspark.

>4. Como o objetivo √© fazer uma an√°lise comparativa, tome em considera√ß√£o o tempo que demora cada etapa, para depois facilitar as compara√ß√µes. 

## üìù Descri√ß√£o do Projeto

#### 1. Defini√ß√£o da estutura da Extra√ß√£o:
Extra√ß√£o foi executada utilziando arquivo formato csv presente no kaggle

#### 2. Trasforma√ß√£o executadas:
Tabelas foram saneadas com plipeza de valores nulos ajustado tipo e verifica√ßo de valores duplicados ou/ e invalidos

#### 3. Persist√™ncia dos dados:
A persist√™ncia dos dados, apos o processo, foi feita atrav√©s de arquivos parquet.

## üì∫ Demonstra√ß√£o

<p align="center">
  <img src="./_captures/Demonstracao.gif">
</p>

## ‚òëÔ∏è  Pr√©-requisitos
- Cadastro no **[Databricks Community](https://www.databricks.com/try-databricks#account)**;

## ‚öôÔ∏è Passo a passo para executar o projeto:
1. Cadastro e login no **[Databricks Community](https://community.cloud.databricks.com/login.html)**;
2. Import de todos os notebooks presentes nesse projeto para o workspace do Databricks:
    - Pode baixar todos os notebooks e importar via browse;
    - Ou pode importar atrav√©s da URL dos notebooks presentes no item [Acesso direto aos notebooks no Databricks](#-acesso-direto-aos-notebooks-no-databricks);
3. Criar e iniciar o cluster no Databricks;
4. Inicializar os notebooks 'Projeto_Bigdata_Pandas' com 'Run All';
5. Inicializar os notebooks 'Projeto_Bigdata_Spark' com 'Run All';

## üõ†Ô∏è Tecnologias Utilizadas

* [Databricks Community](https://www.databricks.com/try-databricks#account)
* [Python](https://www.python.org/) - Linguagem de Programa√ß√£o
* [Spark](https://spark.apache.org/) - Processamento em cluster
* [Pandas](https://pandas.pydata.org/) - Biblioteca de processamento de dados


## üö® Dificuldades
- Trabalhar com Spark e Databricks;
- Encontrar uma solu√ß√£o para guardar os tempos de execu√ß√£o.


## üìà Melhorias futuras:
- Estrutura carregamento de dados utilizando formato tabela floco de neve;
- Realizar jun√ß√£o de tabelas utilziando formato suportado SQL;
- Fazer compara√ß√£o a n√≠vel de mem√≥ria ao inv√©s de apenas tempo de execu√ß√£o.

## üÜó Resultados obtidos:

- Pandas: 

| Processo       | Transa√ß√µes  | Cart√µes   | Usu√°rios  |
|----------------|-------------|-----------|-----------|
| Extracao       | 23.103574   | 0.030902  | 0.010606  |
| Transformacao  | 51.888618   | 0.079121  | 0.071062  |
| Carga          | 8.566636    | 0.086188  | 0.008319  |

- Spark:

| Processo       | Transa√ß√µes  | Cart√µes   | Usu√°rios  |
|----------------|-------------|-----------|-----------|
| Extracao       | 39.579492   | 1.598298  | 1.265999  |
| Transformacao  | 0.011337    | 0.037235  | 0.032321  |
| Carga          | 106.595586  | 2.165236  | 1.719501  |





